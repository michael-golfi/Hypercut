package dbpart.spark

import org.apache.spark.SparkConf
import org.apache.spark.graphx._
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.SparkSession

import com.google.common.hash.Hashing

import dbpart._
import dbpart.bucket._
import dbpart.hash._
import miniasm.genome.util.DNAHelpers

/**
 * Helper routines for executing Hypercut from Apache Spark.
 */
class Routines(spark: SparkSession) {
  implicit val sc: org.apache.spark.SparkContext = spark.sparkContext
  import spark.sqlContext.implicits._
  import dbpart.bucket.CountingSeqBucket._

  /**
   * Load reads and their reverse complements from DNA files.
   */
  def getReads(fileSpec: String): Dataset[String] = {
    val reads = sc.textFile(fileSpec).toDF.map(_.getString(0))
    val withRev = reads.flatMap(r => Seq(r, DNAHelpers.reverseComplement(r)))
    withRev
  }

  def countFeatures(reads: Dataset[String], space: MarkerSpace) = {
    reads.map(r => {
      val c = new FeatureCounter
      val s = new FeatureScanner(space)
      s.scanRead(c, r)
      c
    }).reduce( _+_ )
  }

  def hashReads(reads: Dataset[String], ext: MarkerSetExtractor): Dataset[(CompactNode, String)] = {
    reads.flatMap(r => ext.compactMarkers(r))
  }

  type ProcessedRead = Array[(Array[Byte], String)]

  /**
   * Process a set of reads, generating an intermediate dataset that contains both read segments
   * and edges between buckets.
   */
  def splitReads(reads: Dataset[String],  ext: MarkerSetExtractor): Dataset[ProcessedRead] = {
    reads.map(r => {
      val buckets = ext.markerSetsInRead(r)._2
      ext.splitRead(r, buckets).iterator.map(x => (x._1.compact.data, x._2)).toArray
    })
  }

  /**
   * Extract only segments from the dataset generated by the function above.
   */
  def segmentsFromSplit(reads: Dataset[ProcessedRead]) = reads.flatMap(x => x)

  /**
   * Extract only edges from the dataset generated by the function above.
   */
  def edgesFromSplit(reads: Dataset[ProcessedRead]) = reads.flatMap(s =>
    MarkerSetExtractor.collectTransitions(s.toList.map(_._1)))

  def hashToBuckets[A](reads: Dataset[Array[(Long, String)]], ext: MarkerSetExtractor): Dataset[(Long, SimpleCountingBucket)] = {
    val split = reads.flatMap(x => x)
    val countedSegments =
      split.groupByKey(x => x).mapValues(_._2).count

    val byBucket = countedSegments.groupByKey( { case (key, count) => key._1 }).
      mapValues( { case (key, count) => (key._2, count) })
    val buckets = byBucket.mapGroups(
      { case (bucket, data) => {
        val segmentsCounts = data.toSeq
        val empty = SimpleCountingBucket.empty(ext.k)
        (bucket, empty.insertBulkSegments(segmentsCounts.map(_._1), segmentsCounts.map(c => clipCov(c._2))))
      }
    })
    buckets
  }

  type BucketGraph = Graph[SimpleCountingBucket, Int]
  type PathGraph = Graph[PathNode, Int]

  /**
   * Construct a SparkX graph where the buckets are vertices.
   */
  def bucketGraph(reads: Dataset[ProcessedRead], ext: MarkerSetExtractor): BucketGraph = {
    val inner = new InnerRoutines
    val md5Hashed = reads.map(_.map(s => (inner.longId(s._1), s._2)))
    md5Hashed.cache
    val edges = md5Hashed.flatMap(r => MarkerSetExtractor.collectTransitions(r.toList.map(_._1))).distinct().rdd
    val verts = hashToBuckets(md5Hashed, ext)
    val r = Graph.fromEdgeTuples(edges, 0).outerJoinVertices(verts.rdd)((vid, data, optBucket) => optBucket.get)
    r.cache
    r.numVertices //Force computation
    md5Hashed.unpersist
    r
  }

  def toPathGraph(graph: BucketGraph, k: Int): PathGraph = {
    val inner = new InnerRoutines
    val edges = graph.triplets.flatMap(tr => {
      val fromNodes = tr.srcAttr.sequencesWithCoverage.map(x => new PathNode(x._1, x._2))
      val toNodes = tr.dstAttr.sequencesWithCoverage.map(x => new PathNode(x._1, x._2))
      val edges = (for {
        from <- fromNodes; to <- toNodes
        if from.end(k) == to.begin(k)
        edge = (inner.longId(from), inner.longId(to))
      }  yield edge)
      edges
    })

    val verts = graph.vertices.flatMap(v => v._2.sequencesWithCoverage.map(x =>
      (inner.longId(x._1), new PathNode(x._1, x._2))))
    val r = Graph.fromEdgeTuples(edges, 0).outerJoinVertices(verts)((vid, data, optBucket) => optBucket.get)
    r.cache
    r.numVertices
    r
  }
}

final class InnerRoutines extends Serializable {
  def longId(data: Array[Byte]): Long =
    Hashing.md5.hashBytes(data).asLong

  def longId(data: String): Long =
    Hashing.md5.hashString(data).asLong

  def longId(node: PathNode): Long =
    longId(node.seq)
}

object BuildGraph {
  def conf: SparkConf = {
    val conf = new SparkConf
    conf.set("spark.kryo.classesToRegister", "dbpart.bucket.SimpleCountingBucket,dbpart.hash.CompactNode,dbpart.spark.PathNode")
    GraphXUtils.registerKryoClasses(conf)
    conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    conf
  }

  def main(args: Array[String]) {
    val spark = SparkSession.builder().appName("Hypercut").
      master("spark://localhost:7077").config(conf).getOrCreate()
    val file = args(0)
    val routines = new Routines(spark)
    val reads = routines.getReads(file)
    val ext = MarkerSetExtractor.fromSpace("mixedTest", 5, 41)
    val split = routines.splitReads(reads, ext)
    val g = routines.bucketGraph(split, ext)
    println(g.numVertices)
    val g2 = routines.toPathGraph(g, 41)
    println(g2.numVertices)
    println(g2.numEdges)
  }
}
