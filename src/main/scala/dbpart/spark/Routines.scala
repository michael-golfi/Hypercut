package dbpart.spark

import org.apache.spark.sql.Dataset
import org.apache.spark.sql.SparkSession
import org.apache.spark.graphx._

import dbpart._
import dbpart.bucket._
import dbpart.hash._
import miniasm.genome.util.DNAHelpers

import scala.collection.JavaConverters._
import dbpart.bucketdb.PackedSeqBucket

import com.google.common.hash.Hashing

/**
 * Helper routines for executing Hypercut from Apache Spark.
 */
class Routines(spark: SparkSession) {
  implicit val sc: org.apache.spark.SparkContext = spark.sparkContext
  import spark.sqlContext.implicits._
  import CountingSeqBucket._

  /**
   * Load reads and their reverse complements from DNA files.
   */
  def getReads(fileSpec: String): Dataset[String] = {
    val reads = sc.textFile(fileSpec).toDF.map(_.getString(0))
    val withRev = reads.flatMap(r => Seq(r, DNAHelpers.reverseComplement(r)))
    withRev
  }

  def countFeatures(reads: Dataset[String], space: MarkerSpace) = {
    reads.map(r => {
      val c = new FeatureCounter
      val s = new FeatureScanner(space)
      s.scanRead(c, r)
      c
    }).reduce( _+_ )
  }

  def hashReads(reads: Dataset[String], ext: MarkerSetExtractor): Dataset[(CompactNode, String)] = {
    reads.flatMap(r => ext.compactMarkers(r))
  }

  type ProcessedRead = List[(Array[Byte], String)]

  /**
   * Process a set of reads, generating an intermediate dataset that contains both read segments
   * and edges between buckets.
   */
  def splitReads(reads: Dataset[String],  ext: MarkerSetExtractor): Dataset[ProcessedRead] = {
    val readSet = reads.map(r => {
      val buckets = ext.markerSetsInRead(r)._2
      val segments = ext.splitRead(r, buckets).iterator.map(x => (x._1.compact.data, x._2)).toList
      segments
    })
    readSet
  }

  /**
   * Extract only segments from the dataset generated by the function above.
   */
  def segmentsFromSplit(reads: Dataset[ProcessedRead]) = reads.flatMap(x => x)

  /**
   * Extract only edges from the dataset generated by the function above.
   */
  def edgesFromSplit(reads: Dataset[ProcessedRead]) =
    reads.flatMap(x => MarkerSetExtractor.collectTransitions(x.map(_._1))).distinct()

  def hashToBuckets(reads: Dataset[ProcessedRead], ext: MarkerSetExtractor): Dataset[(Array[Byte], SimpleCountingBucket)] = {
    val split = segmentsFromSplit(reads)
    val countedSegments =
      split.groupByKey(x => (x._1, x._2)).mapValues(_._2).count

    val byBucket = countedSegments.groupByKey( { case (key, count) => key._1 }).
      mapValues( { case (key, count) => (key._2, count) })
    val buckets = byBucket.mapGroups(
      { case (bucket, data) => {
        val segmentsCounts = data.toSeq
        val empty = SimpleCountingBucket.empty(ext. k)
        (bucket, empty.insertBulkSegments(segmentsCounts.map(_._1), segmentsCounts.map(c => clipCov(c._2))))
      }
    })
    buckets
  }

  /**
   * Construct a SparkX graph where the buckets are vertices.
   */
  def makeGraph(reads: Dataset[ProcessedRead], ext: MarkerSetExtractor) = {
    val inner = new InnerRoutines
    reads.cache
    val edges = edgesFromSplit(reads)
    val verts = hashToBuckets(reads, ext).map(v => (inner.longId(v._1), v._2))
    val ids = edges.map(e => (inner.longId(e._1), inner.longId(e._2))).rdd
    val r = Graph.fromEdgeTuples(ids, 0).outerJoinVertices(verts.rdd)((vid, data, optBucket) => optBucket.get)
    r.cache
    r.numVertices //Force computation
    reads.unpersist
    r
  }

  def writeEdges(edges: Dataset[(Array[Byte], Array[Byte])], ext: MarkerSetExtractor, dbFile: String) {
    val d = edges.cache
    val data = d.groupByKey(_._1)
    val n = data.keys.count
    val set = Settings.settings(dbFile, n.toInt)
    val db = set.edgeDb(ext.space)
    val it = data.mapGroups((g, vs) => (g, vs.map(_._2).toList)).toLocalIterator()
    for {
      g <- it.asScala.grouped(1000000)
      m = g.map(x => (x._1, db.newBucket(x._2)))
    } {
      db.overwriteBulk(m)
    }
    db.close
    d.unpersist
  }

  def writeBuckets(buckets: Dataset[(Array[Byte], SimpleCountingBucket)], ext: MarkerSetExtractor, dbFile: String) {
    val data = buckets.cache
    val n = data.count
    val set = Settings.settings(dbFile, n.toInt)
    val db = set.writeBucketDb(ext.k)
    val it = data.toLocalIterator
     for {
      g <- it.asScala.grouped(1000000)
      mg = g.map(x =>
        (MarkerSet.uncompactToString(x._1, ext.space) ->
        PackedSeqBucket(x._2.sequences, x._2.coverages, x._2.k)))
      m = mg.toMap
    } {
      db.overwriteBulk(m)
    }
    db.close
    data.unpersist
  }

  def writeEdgesAndBuckets(reads: Dataset[String], ext: MarkerSetExtractor, dbFile: String) {
    val segments = splitReads(reads, ext)
    writeEdges(edgesFromSplit(segments), ext, dbFile)
    writeBuckets(hashToBuckets(segments, ext), ext, dbFile)
  }
}

class InnerRoutines extends Serializable {
  def longId(data: Array[Byte]) =
    Hashing.md5.hashBytes(data).asLong
}

object BuildGraph {
  def main() {

  }
}
