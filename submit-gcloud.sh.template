#!/bin/bash
#Script to submit to a GCloud dataproc cluster. Copy this file to submit-gcloud.sh and 
#edit variables accordingly.

REGION=asia-northeast1
#Set checkpoint directory, e.g. a GS bucket
CHECKPOINT=gs://path/to/checkpoint

#The first argument is the cluster ID. The remaining arguments will be passed to the Hypercut driver process.
CLUSTER=$1
shift

#The special characters at the start make '##' the separator of properties in the list, since the comma sign
#is needed for spark.jars.packages.
#spark.executor.memory=8g is suitable for highmem nodes on GCloud.
PROPERTIES="^##^spark.jars.packages=org.rogach:scallop_2.11:latest.integration,graphframes:graphframes:0.7.0-spark2.4-s_2.11,org.vegas-viz:vegas_2.11:0.3.11##spark.executor.memory=8g"

exec gcloud --verbosity=info  dataproc jobs submit spark --region asia-northeast1 --cluster $CLUSTER --class hypercut.spark.Hypercut --jars ~/ws/l5/DBPart/target/scala-2.11/hypercut_2.11-0.1.0-SNAPSHOT.jar   --properties $PROPERTIES -- --checkpoint $CHECKPOINT "$@"


